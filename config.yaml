# Discord LLM Bot Configuration

# Discord settings
discord:
  # Leave empty to use value from .env file
  token: ""
  # Command prefix for bot commands
  command_prefix: "/"

# LLM API settings
llm:
  # API settings
  api:
    # URL for the Ollama API
    url: "http://localhost:11434/api/chat"
    # Model name to use with Ollama
    model: "llama3"
  
  # Message settings
  message:
    # Maximum context length (in tokens)
    max_context_length: 4096
    # Maximum response length (in tokens)
    max_length: 800
    # Temperature (randomness)
    temperature: 0.7
    # Top p (nucleus sampling)
    top_p: 0.9
    # Whether to stream responses
    stream: true

# Character settings
character:
  # Path to character.json file
  path: "character.json" 