# Discord LLM Bot Configuration

# Discord settings
discord:
  # Leave empty to use value from .env file
  token: ""
  # Command prefix for bot commands
  command_prefix: "/"

# LLM API settings
llm:
  # API settings
  api:
    # API type (ollama or koboldcpp)
    type: "ollama"
    
    # Ollama settings (used when type is "ollama")
    # URL for the Ollama API
    url: "http://localhost:11434/api/chat"
    # Model name to use with Ollama
    model: "llama3"
    
    # Koboldcpp settings (used when type is "koboldcpp")
    # koboldcpp_url: "http://localhost:5001/v1"
    # API key for Koboldcpp (can be left blank if not set)
    # koboldcpp_api_key: ""
    # Model name to use with Koboldcpp
    # koboldcpp_model: "llama3"
  
  # Message settings
  message:
    # Maximum context length (in tokens)
    max_context_length: 4096
    # Maximum response length (in tokens)
    max_length: 800
    # Temperature (randomness)
    temperature: 0.7
    # Top p (nucleus sampling)
    top_p: 0.9
    # Whether to stream responses
    stream: true

# Character settings
character:
  # Path to character.json file
  path: "character.json" 